{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40956de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing columns in the dataset:\n",
      "['timestamp', 'data_timestamp', 'date', 'buildingid', 'floorid', 'resourceid', 'resourcetype', 'temperature_value', 'temperature_rating', 'light_value', 'light_rating', 'co2_value', 'co2_rating', 'humidity_value', 'humidity_rating', 'radon_value', 'radon_rating', 'airquality_value', 'airquality_rating', 'type', 'bookable', 'size']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = \"/Users/alexo/Desktop/VIP/data/data_sony_clean.csv\"\n",
    "\n",
    "# Read just the first row to get columns\n",
    "df_header = pd.read_csv(data_path, nrows=1)\n",
    "columns = df_header.columns.tolist()\n",
    "\n",
    "print(\"Existing columns in the dataset:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04223e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (100000, 22)\n",
      "\n",
      "Data types:\n",
      " timestamp              object\n",
      "data_timestamp         object\n",
      "date                   object\n",
      "buildingid              int64\n",
      "floorid                 int64\n",
      "resourceid             object\n",
      "resourcetype           object\n",
      "temperature_value     float64\n",
      "temperature_rating     object\n",
      "light_value           float64\n",
      "light_rating           object\n",
      "co2_value             float64\n",
      "co2_rating             object\n",
      "humidity_value        float64\n",
      "humidity_rating        object\n",
      "radon_value           float64\n",
      "radon_rating           object\n",
      "airquality_value      float64\n",
      "airquality_rating      object\n",
      "type                   object\n",
      "bookable                 bool\n",
      "size                  float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      " timestamp             0\n",
      "data_timestamp        0\n",
      "date                  0\n",
      "buildingid            0\n",
      "floorid               0\n",
      "resourceid            0\n",
      "resourcetype          0\n",
      "temperature_value     0\n",
      "temperature_rating    0\n",
      "light_value           0\n",
      "light_rating          0\n",
      "co2_value             0\n",
      "co2_rating            0\n",
      "humidity_value        0\n",
      "humidity_rating       0\n",
      "radon_value           0\n",
      "radon_rating          0\n",
      "airquality_value      0\n",
      "airquality_rating     0\n",
      "type                  0\n",
      "bookable              0\n",
      "size                  0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics:\n",
      "        buildingid   floorid  temperature_value    light_value      co2_value  \\\n",
      "count    100000.0  100000.0      100000.000000  100000.000000  100000.000000   \n",
      "mean       6074.0    1394.0          21.893132      18.141720     510.121350   \n",
      "std           0.0       0.0           0.716060      21.211345     110.553514   \n",
      "min        6074.0    1394.0          17.700000       0.000000     391.000000   \n",
      "25%        6074.0    1394.0          21.700000       0.000000     465.000000   \n",
      "50%        6074.0    1394.0          21.900000       1.000000     483.000000   \n",
      "75%        6074.0    1394.0          22.200000      41.000000     510.000000   \n",
      "max        6074.0    1394.0          26.500000      74.000000    5844.000000   \n",
      "\n",
      "       humidity_value    radon_value  airquality_value           size  \n",
      "count   100000.000000  100000.000000     100000.000000  100000.000000  \n",
      "mean        29.664280      12.748800         99.171750       4.871320  \n",
      "std          3.780126       6.521663          6.394238       2.987817  \n",
      "min         20.000000       0.000000          0.000000       2.000000  \n",
      "25%         27.000000       8.000000        100.000000       4.000000  \n",
      "50%         29.000000      12.000000        100.000000       4.000000  \n",
      "75%         32.000000      16.000000        100.000000       4.000000  \n",
      "max         78.000000      47.000000        100.000000      12.000000  \n",
      "\n",
      "Number of duplicate rows in sample: 48\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = \"/Users/alexo/Desktop/VIP/data/data_sony_clean.csv\"\n",
    "\n",
    "# Load a sample (first 100,000 rows) for exploration to handle large size\n",
    "df_sample = pd.read_csv(\n",
    "    data_path, nrows=100000, parse_dates=[\"timestamp\"]\n",
    ")  # Adjust 'timestamp' if named differently\n",
    "\n",
    "# Basic info\n",
    "print(\"Sample shape:\", df_sample.shape)\n",
    "print(\"\\nData types:\\n\", df_sample.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df_sample.isnull().sum())\n",
    "print(\"\\nSummary statistics:\\n\", df_sample.describe())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_sample.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in sample: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8626c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Original shape: (702169, 22)\n",
      "Columns: ['timestamp', 'data_timestamp', 'date', 'buildingid', 'floorid', 'resourceid', 'resourcetype', 'temperature_value', 'temperature_rating', 'light_value', 'light_rating', 'co2_value', 'co2_rating', 'humidity_value', 'humidity_rating', 'radon_value', 'radon_rating', 'airquality_value', 'airquality_rating', 'type', 'bookable', 'size']\n",
      "\n",
      "Selecting features...\n",
      "Cleaning and sorting...\n",
      "Warning: Dropping 720 rows with unparsable timestamps\n",
      "Normalizing sensor values per resource...\n",
      "Saving to ../../data/vae_data.csv...\n",
      "\n",
      "VAE dataset ready!\n",
      "   Shape : (701115, 8)\n",
      "   File  : /Users/alexo/Desktop/VIP/github-repo/VIP/data/vae_data.csv\n",
      "                         timestamp                            resourceid  \\\n",
      "0 2024-02-11 23:59:41.269000+00:00  12834a49-ce1d-4f43-949a-996dbb088f74   \n",
      "1 2024-02-11 23:59:41.323000+00:00  12834a49-ce1d-4f43-949a-996dbb088f74   \n",
      "2 2024-02-12 00:04:44.262000+00:00  12834a49-ce1d-4f43-949a-996dbb088f74   \n",
      "3 2024-02-12 00:04:44.336000+00:00  12834a49-ce1d-4f43-949a-996dbb088f74   \n",
      "4 2024-02-12 00:09:41.306000+00:00  12834a49-ce1d-4f43-949a-996dbb088f74   \n",
      "\n",
      "   temperature_value  humidity_value  co2_value  light_value  radon_value  \\\n",
      "0                0.0        0.183673   0.051982          0.0     0.177215   \n",
      "1                0.0        0.183673   0.051982          0.0     0.177215   \n",
      "2                0.0        0.183673   0.073128          0.0     0.177215   \n",
      "3                0.0        0.183673   0.073128          0.0     0.177215   \n",
      "4                0.0        0.183673   0.067841          0.0     0.177215   \n",
      "\n",
      "   airquality_value  \n",
      "0               1.0  \n",
      "1               1.0  \n",
      "2               1.0  \n",
      "3               1.0  \n",
      "4               1.0  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Create VAE-ready dataset from pre-processed Sony Nimway data.\n",
    "Input : /Users/alexo/Desktop/VIP/data/data_sony_clean.csv\n",
    "Output: ../../data/vae_data.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "INPUT_PATH = Path(\"/Users/alexo/Desktop/VIP/data/data_sony_clean.csv\")\n",
    "OUTPUT_DIR = Path(\"../../data\")\n",
    "OUTPUT_PATH = OUTPUT_DIR / \"vae_data.csv\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SENSOR_COLS = [\n",
    "    \"temperature_value\",\n",
    "    \"humidity_value\",\n",
    "    \"co2_value\",\n",
    "    \"light_value\",\n",
    "    \"radon_value\",\n",
    "    \"airquality_value\",\n",
    "]\n",
    "CONTEXT_COLS = [\"timestamp\", \"resourceid\"]\n",
    "FINAL_COLS = CONTEXT_COLS + SENSOR_COLS\n",
    "\n",
    "# -------------------------------\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(INPUT_PATH, parse_dates=[\"timestamp\"])  # <-- ISO-8601 works here\n",
    "\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Feature Selection\n",
    "# -------------------------------\n",
    "print(\"\\nSelecting features...\")\n",
    "missing = [c for c in FINAL_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "df = df[FINAL_COLS].copy()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Cleaning & Sorting\n",
    "# -------------------------------\n",
    "print(\"Cleaning and sorting...\")\n",
    "df = (\n",
    "    df.drop_duplicates().sort_values([\"resourceid\", \"timestamp\"]).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---- FIXED TIMESTAMP PARSING (only needed if read_csv did not parse it) ----\n",
    "df[\"timestamp\"] = pd.to_datetime(\n",
    "    df[\"timestamp\"], utc=True, errors=\"coerce\", infer_datetime_format=True\n",
    ")\n",
    "n_bad = df[\"timestamp\"].isna().sum()\n",
    "if n_bad:\n",
    "    print(f\"Warning: Dropping {n_bad} rows with unparsable timestamps\")\n",
    "    df = df.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Per-resource Normalization (MinMax [0,1])\n",
    "# -------------------------------\n",
    "print(\"Normalizing sensor values per resource...\")\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "def normalize_group(g):\n",
    "    sensors = g[SENSOR_COLS]\n",
    "    if sensors.isnull().all().all():\n",
    "        return g\n",
    "    scaled = scaler.fit_transform(sensors)\n",
    "    g = g.copy()\n",
    "    g[SENSOR_COLS] = pd.DataFrame(scaled, columns=SENSOR_COLS, index=g.index)\n",
    "    return g\n",
    "\n",
    "\n",
    "df_norm = df.groupby(\"resourceid\", group_keys=False).apply(normalize_group)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Final clean-up\n",
    "# -------------------------------\n",
    "df_norm[SENSOR_COLS] = df_norm[SENSOR_COLS].ffill().bfill()\n",
    "df_final = df_norm.dropna(subset=SENSOR_COLS, how=\"any\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save\n",
    "# -------------------------------\n",
    "print(f\"Saving to {OUTPUT_PATH}...\")\n",
    "df_final.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nVAE dataset ready!\")\n",
    "print(f\"   Shape : {df_final.shape}\")\n",
    "print(f\"   File  : {OUTPUT_PATH.resolve()}\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e57e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vae_data.csv ...\n",
      "   rows before: 701,115\n",
      "   duplicates removed: 0\n",
      "   rows after : 701,115\n",
      "\n",
      "File overwritten → /Users/alexo/Desktop/VIP/github-repo/VIP/data/vae_data.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "#  Remove duplicates from vae_data.csv (in-place)\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- 1. Path (adjust only if you move the notebook) ----\n",
    "CSV_PATH = Path(\"/Users/alexo/Desktop/VIP/github-repo/VIP/data/vae_data.csv\")\n",
    "\n",
    "# ---- 2. Load ------------------------------------------------\n",
    "print(f\"Loading {CSV_PATH.name} ...\")\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "print(f\"   rows before: {len(df):,}\")\n",
    "\n",
    "# ---- 3. Drop duplicates ------------------------------------\n",
    "#   * keep='first' → retain the first occurrence\n",
    "#   * subset=None  → compare **all** columns\n",
    "df_clean = df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# ---- 4. Summary ---------------------------------------------\n",
    "dup_count = len(df) - len(df_clean)\n",
    "print(f\"   duplicates removed: {dup_count:,}\")\n",
    "print(f\"   rows after : {len(df_clean):,}\")\n",
    "\n",
    "# ---- 5. Overwrite the original file -------------------------\n",
    "df_clean.to_csv(CSV_PATH, index=False)\n",
    "print(f\"\\nFile overwritten → {CSV_PATH}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c002921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vae_data.csv...\n",
      "   Rows: 701,115\n",
      "Sorting and checking time gaps per room...\n",
      "   Irregular gaps (>30 min): 169 rows in 9 rooms\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "#  PREPROCESSING FOR ADAPTIVE VAE (ONLINE + HUMAN-IN-LOOP)\n",
    "#  Input : vae_data.csv\n",
    "#  Output: vae_final_streaming.csv + scaler_per_room.pkl\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Cleaned VAE Data\n",
    "# -------------------------------\n",
    "CSV_PATH = Path(\"/Users/alexo/Desktop/VIP/github-repo/VIP/data/vae_data.csv\")\n",
    "print(f\"Loading {CSV_PATH.name}...\")\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[\"timestamp\"])\n",
    "print(f\"   Rows: {len(df):,}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Sanity Checks\n",
    "# -------------------------------\n",
    "assert df.duplicated().sum() == 0, \"Duplicates still present!\"\n",
    "assert df[\"timestamp\"].isna().sum() == 0, \"Missing timestamps!\"\n",
    "\n",
    "SENSOR_COLS = [\n",
    "    \"temperature_value\",\n",
    "    \"humidity_value\",\n",
    "    \"co2_value\",\n",
    "    \"light_value\",\n",
    "    \"radon_value\",\n",
    "    \"airquality_value\",\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Sort & Time Gaps\n",
    "# -------------------------------\n",
    "print(\"Sorting and checking time gaps per room...\")\n",
    "df = df.sort_values([\"resourceid\", \"timestamp\"]).reset_index(drop=True)\n",
    "df[\"time_diff_min\"] = (\n",
    "    df.groupby(\"resourceid\")[\"timestamp\"].diff().dt.total_seconds() / 60\n",
    ")\n",
    "irregular = df[df[\"time_diff_min\"] > 30]\n",
    "print(\n",
    "    f\"   Irregular gaps (>30 min): {len(irregular):,} rows in {irregular['resourceid'].nunique()} rooms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42b94c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling sensors per resourceid...\n",
      "   Scalers saved for 9 rooms\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. Per-Room MinMax Scaling + Save Scalers\n",
    "# -------------------------------\n",
    "print(\"Scaling sensors per resourceid...\")\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def scale_group(group):\n",
    "    rid = group[\"resourceid\"].iloc[0]\n",
    "    scaler = MinMaxScaler()\n",
    "    sensors = group[SENSOR_COLS].copy()\n",
    "    scaled = scaler.fit_transform(sensors)\n",
    "    group[SENSOR_COLS] = pd.DataFrame(scaled, columns=SENSOR_COLS, index=group.index)\n",
    "    scalers[rid] = scaler\n",
    "    return group\n",
    "\n",
    "\n",
    "df_scaled = df.groupby(\"resourceid\", group_keys=False).apply(scale_group)\n",
    "print(f\"   Scalers saved for {len(scalers)} rooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "653000c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating small gaps per room...\n",
      "   Interpolation complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Interpolate Small Gaps (Per Room)\n",
    "# -------------------------------\n",
    "print(\"Interpolating small gaps per room...\")\n",
    "df_interp = df_scaled.copy()\n",
    "\n",
    "# Only interpolate within each room\n",
    "for rid, group in df_interp.groupby(\"resourceid\"):\n",
    "    df_interp.loc[group.index, SENSOR_COLS] = group[SENSOR_COLS].interpolate(\n",
    "        method=\"linear\", limit_direction=\"both\"\n",
    "    )\n",
    "\n",
    "# Final NaN check\n",
    "assert df_interp[SENSOR_COLS].isna().sum().sum() == 0, \"NaNs remain!\"\n",
    "print(\"   Interpolation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b844adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sliding windows (size=10)...\n",
      "   Sequences created: 701,034\n",
      "   Features per sequence: 63\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6. Create Sliding Window Sequences (VAE Input)\n",
    "# -------------------------------\n",
    "WINDOW_SIZE = 10  # 10 timesteps (~50 min if 5-min data)\n",
    "print(f\"Creating sliding windows (size={WINDOW_SIZE})...\")\n",
    "\n",
    "sequences = []\n",
    "\n",
    "\n",
    "def create_sequences(group):\n",
    "    if len(group) < WINDOW_SIZE:\n",
    "        return\n",
    "    data = group[SENSOR_COLS].values\n",
    "    timestamps = group[\"timestamp\"].values\n",
    "    resource_id = group[\"resourceid\"].iloc[0]\n",
    "\n",
    "    for i in range(len(data) - WINDOW_SIZE + 1):\n",
    "        seq = data[i : i + WINDOW_SIZE]\n",
    "        row = {\n",
    "            \"seq_id\": f\"{resource_id}_{i}\",\n",
    "            \"resourceid\": resource_id,\n",
    "            \"start_time\": pd.Timestamp(timestamps[i]),\n",
    "        }\n",
    "        # Flatten sequence: t0_feature1, t0_feature2, ..., t9_feature6\n",
    "        for t in range(WINDOW_SIZE):\n",
    "            for f_idx, feature in enumerate(SENSOR_COLS):\n",
    "                row[f\"{feature}_t{t}\"] = seq[t, f_idx]\n",
    "        sequences.append(row)\n",
    "\n",
    "\n",
    "# Apply per room\n",
    "_ = df_interp.groupby(\"resourceid\", group_keys=False).apply(create_sequences)\n",
    "\n",
    "df_seq = pd.DataFrame(sequences)\n",
    "print(f\"   Sequences created: {len(df_seq):,}\")\n",
    "print(f\"   Features per sequence: {len(df_seq.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0209092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREPROCESSING COMPLETE!\n",
      "   → Sequences: /Users/alexo/Desktop/VIP/github-repo/VIP/data/vae_final_streaming.csv\n",
      "   → Scalers:   /Users/alexo/Desktop/VIP/github-repo/VIP/data/scaler_per_room.pkl\n",
      "   → Total sequences: 701,034\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 7. Save Final Files\n",
    "# -------------------------------\n",
    "OUTPUT_DIR = CSV_PATH.parent\n",
    "FINAL_CSV = OUTPUT_DIR / \"vae_final_streaming.csv\"\n",
    "SCALER_PKL = OUTPUT_DIR / \"scaler_per_room.pkl\"\n",
    "\n",
    "df_seq.to_csv(FINAL_CSV, index=False)\n",
    "with open(SCALER_PKL, \"wb\") as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(f\"\\nPREPROCESSING COMPLETE!\")\n",
    "print(f\"   → Sequences: {FINAL_CSV}\")\n",
    "print(f\"   → Scalers:   {SCALER_PKL}\")\n",
    "print(f\"   → Total sequences: {len(df_seq):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
